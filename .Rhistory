library(caret)
confusionMatrix(factor(test_model$cluster, levels = 1:3), factor(dataset$Species, levels = 1:3))
set.seed(12)
k <- 3
test_model <- kmeans(flower_features, centers = k, nstart = 20)
dataset$cluster_id <- factor(test_model$cluster)
ggplot(dataset, aes(Sepal.Length, Sepal.Width, color = cluster_id)) +
geom_point(alpha = 0.5) +
xlab("Sepal Length") +
ylab("Sepal Width")
kmean_model_cluster <- as.data.frame(test_model$cluster)
#install.packages('caret')
library(caret)
confusionMatrix(factor(test_model$cluster, levels = 1:3), factor(dataset$Species, levels = 1:3))
set.seed(1)
k <- 3
test_model <- kmeans(flower_features, centers = k, nstart = 20)
dataset$cluster_id <- factor(test_model$cluster)
ggplot(dataset, aes(Sepal.Length, Sepal.Width, color = cluster_id)) +
geom_point(alpha = 0.5) +
xlab("Sepal Length") +
ylab("Sepal Width")
kmean_model_cluster <- as.data.frame(test_model$cluster)
#install.packages('caret')
library(caret)
confusionMatrix(factor(test_model$cluster, levels = 1:3), factor(dataset$Species, levels = 1:3))
print("Setosa Statistics")
summary(iris[iris$Species == "setosa",])
cat("\n")
print("Versicolor Statistics")
summary(iris[iris$Species == "versicolor",])
cat("\n")
print("Virginica Statistics")
summary(iris[iris$Species == "virginica",])
cat("\n")
test_model$centers
knitr::opts_chunk$set(echo = TRUE)
#We'll be using kknn
dataset <- read.table("hw2-FA23/data 3.1/credit_card_data-headers.txt", stringsAsFactors = FALSE, header = TRUE) #Read in dataset
print(dim(dataset)) #Look at dimensions
head(dataset, 5) #View first 5 rows
library(kknn) #Load kknn
set.seed(1) #To reproduce results
full_df <- data.frame()
for (kernel in c("rectangular", "triangular", "epanechnikov","biweight",
"triweight","cos","inv","gaussian","rank","optimal")){
for (i in 1:25){
cv_model <- cv.kknn(R1~A1+A2+A3+A8+A9+A10+A11+A12+A14+A15, data = dataset, kernel = kernel, kcv = 10, k = i, scale = TRUE)
target <- cv_model[[1]][,1]
pred <- as.integer(cv_model[[1]][,2]+0.5)
accuracy <- sum(target == pred) / nrow(dataset)
test_df <- data.frame("Kernel" = kernel, "K" = i, "Accuracy" = accuracy) #Make new dataframe with columns of Kernels, k values, and Accuracies
full_df <- rbind(full_df, test_df) #Concatenate to original dataset and continuously update.
}
}
dim(full_df) #There should be 250 rows
highest_values <- full_df[full_df$Accuracy > 0.85,]
sorted_high_values <- highest_values[order(highest_values$Accuracy, decreasing = TRUE),]
head(sorted_high_values,20)
set.seed(1)
num_rows <- nrow(dataset) #Count number of rows
index <- sample(1:num_rows, size = round(num_rows/3), replace = FALSE) #Randomly selecting indexes
df_train <- dataset[-index,] #1/3 of dataset
df_test <- dataset[index,] #2/3 of dataset
model <- kknn(R1~A1+A2+A3+A8+A9+A10+A11+A12+A14+A15, train = df_train, test = df_test, kernel = "gaussian", k = 7, scale = TRUE)
pred <- as.integer(model$fitted.values+0.5)
accuracy <- (sum(pred == df_test[,11]) / nrow(df_test)*100)
print(paste0("Our KNN model using a Guassian Kernel and 7 neighbors has an accuracy of: ", round(accuracy,2), "%"))
set.seed(1) #Set again for random numbers
num_rows <- nrow(dataset) #Count number of rows
#Set fractions
fraction_Training <- 0.70
fraction_Validation <- 0.15
fraction_Test <- 0.15
size_Training <- floor(fraction_Training * num_rows) #60% values of dataset
size_Validation <- floor(fraction_Validation * num_rows) #20% values of dataset
size_Test <- floor(fraction_Test * num_rows) #20% values of dataset
#Compute indices
indicesTraining <- sort(sample(seq_len(num_rows), size = size_Training)) #Compute index for training
indicesNotTraining <- setdiff(seq_len(num_rows), indicesTraining) #Find difference in index so there's no overlap
indicesValidation <- sort(sample(indicesNotTraining, size = size_Validation)) #Compute index for validation from non-training indices
indicesTest <- setdiff(indicesNotTraining, indicesValidation) #Finds rest of indices
#Set datasets
dfTraining <- dataset[indicesTraining, ]
dfValidation <- dataset[indicesValidation, ]
dfTest <- dataset[indicesTest, ]
print(paste("Fractions of Training set:", dim(dfTraining)[1] / num_rows))
print(paste("Fractions of Validation set:", dim(dfValidation)[1] / num_rows))
print(paste("Fractions of Testing set:", dim(dfTest)[1] / num_rows))
set.seed(1)
knn_df = data.frame()
for (kernel in c("rectangular", "triangular", "epanechnikov","biweight",
"triweight","cos","inv","gaussian","rank","optimal")){
for (i in 1:25){
model <- kknn(R1~A1+A2+A3+A8+A9+A10+A11+A12+A14+A15, train = dfTraining, test = dfValidation, kernel = kernel, k = i, scale = TRUE)
round_pred <- as.integer(model$fitted.values + 0.5) #Round values to 0 and 1 so we can match the target column
accuracy <- sum(round_pred == dfValidation$R1) / length(dfValidation$R1) #Test accuracy
test_df <- data.frame("Kernel" = kernel, "K" = i, "Accuracy" = accuracy) #Make new dataframe
knn_df <- rbind(knn_df, test_df) #Concatenate
}
}
dim(knn_df) #Should be 250 values
highest_values <- knn_df[knn_df$Accuracy > 0.85,]
sorted_high_values <- highest_values[order(highest_values$Accuracy, decreasing = TRUE),]
head(sorted_high_values,20)
set.seed(1)
model <- kknn(R1~A1+A2+A3+A8+A9+A10+A11+A12+A14+A15, train = dfTraining, test = dfTest, kernel = "rectangular", k = 14, scale = TRUE)
pred <- as.integer(model$fitted.values+0.5)
accuracy <- (sum(pred == dfTest[,11]) / nrow(dfTest)*100)
print(paste0("Our KNN model using a Rectangular Kernel and 14 neighbors has an accuracy of: ", round(accuracy,2), "%"))
library(datasets) #Iris is a dataset in R, so we can just load it
data(iris)
summary(iris)
dataset <- iris
dataset[,5] <- as.numeric(unclass(dataset[,5])) #Convert target column to numbers and numeric for correlation
dataset.cor <- cor(dataset) #Correlation matrix
dataset.cor
set.seed(1)
flower_features <- scale(as.matrix(dataset[,1:4])) #Set predictive variables
n_clusters <- 5
within_sums <- numeric(n_clusters)
set.seed(1)
# Look over 1 to n possible clusters
for (i in 1:5) {
# Fit the model
model <- kmeans(flower_features, centers = i, nstart = 20)
# Save to vector
within_sums[i] <- model$tot.withinss
}
elbow_df <- data.frame(clusters = 1:5, within_sums = within_sums)
library(ggplot2)
ggplot(elbow_df, aes(x = clusters, y = within_sums, group = 1)) +
geom_point(size = 4)+
geom_line() +
xlab('Number of clusters') + ylab("Within Sums") + ggtitle("Picking a Cluster #")
set.seed(1)
k <- 3
test_model <- kmeans(flower_features, centers = k, nstart = 20)
dataset$cluster_id <- factor(test_model$cluster)
ggplot(dataset, aes(Sepal.Length, Sepal.Width, color = cluster_id)) +
geom_point(alpha = 0.5) +
xlab("Sepal Length") +
ylab("Sepal Width")
kmean_model_cluster <- as.data.frame(test_model$cluster)
#install.packages('caret')
library(caret)
confusionMatrix(factor(test_model$cluster, levels = 1:3), factor(dataset$Species, levels = 1:3))
print("Setosa Statistics")
summary(iris[iris$Species == "setosa",])
print("Versicolor Statistics")
summary(iris[iris$Species == "versicolor",])
print("Virginica Statistics")
summary(iris[iris$Species == "virginica",])
test_model$centers
?kmeans
set.seed(1)
k <- 3
test_model <- kmeans(flower_features, centers = k, nstart = 20)
dataset$cluster_id <- factor(test_model$cluster)
ggplot(dataset, aes(Sepal.Length, Sepal.Width, color = cluster_id)) +
geom_point(alpha = 0.5) +
xlab("Sepal Length") +
ylab("Sepal Width")
test_model$withinss
test_model$tot.withinss
test_model$size
set.seed(1)
k <- 3
test_model <- kmeans(flower_features, centers = k, nstart = 20)
dataset$cluster_id <- factor(test_model$cluster)
ggplot(dataset, aes(Sepal.Length, Sepal.Width, color = cluster_id)) +
geom_point(alpha = 0.5) +
xlab("Sepal Length") +
ylab("Sepal Width")
#install.packages('caret')
library(caret)
confusionMatrix(factor(test_model$cluster, levels = 1:3), factor(dataset$Species, levels = 1:3))
#install.packages('caret')
library(caret)
confusionMatrix(factor(dataset$Species, levels = 1:3), factor(test_model$cluster, levels = 1:3))
?confusionMatrix
#install.packages('caret')
#library(caret)
library(ModelMetrics)
#install.packages('caret')
#library(caret)
library(ModelMetrics)
confusionMatrix(factor(dataset$Species, levels = 1:3), factor(test_model$cluster, levels = 1:3))
#confusionMatrix(factor(dataset$Species, levels = 1:3), factor(test_model$cluster, levels = 1:3))
#?confusionMatrix
#install.packages('caret')
#library(caret)
library(ModelMetrics)
confusionMatrix(factor(dataset$Species, levels = 1:3), factor(test_model$cluster, levels = 1:3))
#confusionMatrix(factor(dataset$Species, levels = 1:3), factor(test_model$cluster, levels = 1:3))
#?confusionMatrix
#install.packages('caret')
library(caret)
confusionMatrix(factor(dataset$Species, levels = 1:3), factor(test_model$cluster, levels = 1:3))
#install.packages('caret')
library(caret)
confusionMatrix(factor(dataset$Species, levels = 1:3), factor(test_model$cluster, levels = 1:3))
knitr::opts_chunk$set(echo = TRUE)
#We'll be using kknn
dataset <- read.table("hw2-FA23/data 3.1/credit_card_data-headers.txt", stringsAsFactors = FALSE, header = TRUE) #Read in dataset
print(dim(dataset)) #Look at dimensions
head(dataset, 5) #View first 5 rows
library(kknn) #Load kknn
set.seed(1) #To reproduce results
full_df <- data.frame()
for (kernel in c("rectangular", "triangular", "epanechnikov","biweight",
"triweight","cos","inv","gaussian","rank","optimal")){
for (i in 1:25){
cv_model <- cv.kknn(R1~A1+A2+A3+A8+A9+A10+A11+A12+A14+A15, data = dataset, kernel = kernel, kcv = 10, k = i, scale = TRUE)
target <- cv_model[[1]][,1]
pred <- as.integer(cv_model[[1]][,2]+0.5)
accuracy <- sum(target == pred) / nrow(dataset)
test_df <- data.frame("Kernel" = kernel, "K" = i, "Accuracy" = accuracy) #Make new dataframe with columns of Kernels, k values, and Accuracies
full_df <- rbind(full_df, test_df) #Concatenate to original dataset and continuously update.
}
}
dim(full_df) #There should be 250 rows
highest_values <- full_df[full_df$Accuracy > 0.85,]
sorted_high_values <- highest_values[order(highest_values$Accuracy, decreasing = TRUE),]
head(sorted_high_values,20)
set.seed(1)
num_rows <- nrow(dataset) #Count number of rows
index <- sample(1:num_rows, size = round(num_rows/3), replace = FALSE) #Randomly selecting indexes
df_train <- dataset[-index,] #1/3 of dataset
df_test <- dataset[index,] #2/3 of dataset
model <- kknn(R1~A1+A2+A3+A8+A9+A10+A11+A12+A14+A15, train = df_train, test = df_test, kernel = "gaussian", k = 7, scale = TRUE)
pred <- as.integer(model$fitted.values+0.5)
accuracy <- (sum(pred == df_test[,11]) / nrow(df_test)*100)
print(paste0("Our KNN model using a Guassian Kernel and 7 neighbors has an accuracy of: ", round(accuracy,2), "%"))
set.seed(1) #Set again for random numbers
num_rows <- nrow(dataset) #Count number of rows
#Set fractions
fraction_Training <- 0.70
fraction_Validation <- 0.15
fraction_Test <- 0.15
size_Training <- floor(fraction_Training * num_rows) #60% values of dataset
size_Validation <- floor(fraction_Validation * num_rows) #20% values of dataset
size_Test <- floor(fraction_Test * num_rows) #20% values of dataset
#Compute indices
indicesTraining <- sort(sample(seq_len(num_rows), size = size_Training)) #Compute index for training
indicesNotTraining <- setdiff(seq_len(num_rows), indicesTraining) #Find difference in index so there's no overlap
indicesValidation <- sort(sample(indicesNotTraining, size = size_Validation)) #Compute index for validation from non-training indices
indicesTest <- setdiff(indicesNotTraining, indicesValidation) #Finds rest of indices
#Set datasets
dfTraining <- dataset[indicesTraining, ]
dfValidation <- dataset[indicesValidation, ]
dfTest <- dataset[indicesTest, ]
print(paste("Fractions of Training set:", dim(dfTraining)[1] / num_rows))
print(paste("Fractions of Validation set:", dim(dfValidation)[1] / num_rows))
print(paste("Fractions of Testing set:", dim(dfTest)[1] / num_rows))
set.seed(1)
knn_df = data.frame()
for (kernel in c("rectangular", "triangular", "epanechnikov","biweight",
"triweight","cos","inv","gaussian","rank","optimal")){
for (i in 1:25){
model <- kknn(R1~A1+A2+A3+A8+A9+A10+A11+A12+A14+A15, train = dfTraining, test = dfValidation, kernel = kernel, k = i, scale = TRUE)
round_pred <- as.integer(model$fitted.values + 0.5) #Round values to 0 and 1 so we can match the target column
accuracy <- sum(round_pred == dfValidation$R1) / length(dfValidation$R1) #Test accuracy
test_df <- data.frame("Kernel" = kernel, "K" = i, "Accuracy" = accuracy) #Make new dataframe
knn_df <- rbind(knn_df, test_df) #Concatenate
}
}
dim(knn_df) #Should be 250 values
highest_values <- knn_df[knn_df$Accuracy > 0.85,]
sorted_high_values <- highest_values[order(highest_values$Accuracy, decreasing = TRUE),]
head(sorted_high_values,20)
set.seed(1)
model <- kknn(R1~A1+A2+A3+A8+A9+A10+A11+A12+A14+A15, train = dfTraining, test = dfTest, kernel = "rectangular", k = 14, scale = TRUE)
pred <- as.integer(model$fitted.values+0.5)
accuracy <- (sum(pred == dfTest[,11]) / nrow(dfTest)*100)
print(paste0("Our KNN model using a Rectangular Kernel and 14 neighbors has an accuracy of: ", round(accuracy,2), "%"))
library(datasets) #Iris is a dataset in R, so we can just load it
data(iris)
summary(iris)
dataset <- iris
dataset[,5] <- as.numeric(unclass(dataset[,5])) #Convert target column to numbers and numeric for correlation
dataset.cor <- cor(dataset) #Correlation matrix
dataset.cor
set.seed(1)
flower_features <- scale(as.matrix(dataset[,1:4])) #Set predictive variables
n_clusters <- 5
within_sums <- numeric(n_clusters)
set.seed(1)
# Look over 1 to n possible clusters
for (i in 1:5) {
# Fit the model
model <- kmeans(flower_features, centers = i, nstart = 20)
# Save to vector
within_sums[i] <- model$tot.withinss
}
elbow_df <- data.frame(clusters = 1:5, within_sums = within_sums)
library(ggplot2)
ggplot(elbow_df, aes(x = clusters, y = within_sums, group = 1)) +
geom_point(size = 4)+
geom_line() +
xlab('Number of clusters') + ylab("Within Sums") + ggtitle("Picking a Cluster #")
set.seed(1)
k <- 3
test_model <- kmeans(flower_features, centers = k, nstart = 20)
dataset$cluster_id <- factor(test_model$cluster)
ggplot(dataset, aes(Sepal.Length, Sepal.Width, color = cluster_id)) +
geom_point(alpha = 0.5) +
xlab("Sepal Length") +
ylab("Sepal Width")
#install.packages('caret')
library(caret)
confusion
#install.packages('caret')
library(caret)
confusionMatrix(factor(dataset$Species, levels = 1:3), factor(test_model$cluster, levels = 1:3))
#install.packages('caret')
library(caret)
xtab <- table(test_model$cluster, dataset$Species)
#confusionMatrix(factor(dataset$Species, levels = 1:3), factor(test_model$cluster, levels = 1:3))
confusionMatrix(xtab)
set.seed(5)
k <- 3
test_model <- kmeans(flower_features, centers = k, nstart = 20)
dataset$cluster_id <- factor(test_model$cluster)
ggplot(dataset, aes(Sepal.Length, Sepal.Width, color = cluster_id)) +
geom_point(alpha = 0.5) +
xlab("Sepal Length") +
ylab("Sepal Width")
#install.packages('caret')
library(caret)
confusionMatrix(factor(dataset$Species, levels = 1:3), factor(test_model$cluster, levels = 1:3))
print("Setosa Statistics")
summary(iris[iris$Species == "setosa",])
print("Versicolor Statistics")
summary(iris[iris$Species == "versicolor",])
print("Virginica Statistics")
summary(iris[iris$Species == "virginica",])
test_model$centers
set.seed(5)
k <- 3
test_model <- kmeans(flower_features, centers = k, nstart = 20)
dataset$cluster_id <- factor(test_model$cluster)
ggplot(dataset, aes(Sepal.Length, Sepal.Width, color = cluster_id)) +
geom_point(alpha = 0.5) +
xlab("Sepal Length") +
ylab("Sepal Width")
#install.packages('caret')
library(caret)
confusionMatrix(factor(dataset$Species, levels = 1:3), factor(test_model$cluster, levels = 1:3))
print("Setosa Statistics")
summary(iris[iris$Species == "setosa",])
print("Versicolor Statistics")
summary(iris[iris$Species == "versicolor",])
print("Virginica Statistics")
summary(iris[iris$Species == "virginica",])
test_model$centers
test_model$cluster
test_model$centers
set.seed(5)
k <- 3
test_model <- kmeans(flower_features, centers = k, nstart = 20)
dataset$cluster_id <- factor(test_model$cluster)
ggplot(dataset, aes(Sepal.Length, Sepal.Width, color = cluster_id)) +
geom_point(alpha = 0.5) +
xlab("Sepal Length") +
ylab("Sepal Width")
#install.packages('caret')
library(caret)
confusionMatrix(factor(dataset$Species, levels = 1:3), factor(test_model$cluster, levels = 1:3))
print("Setosa Statistics")
summary(iris[iris$Species == "setosa",])
print("Versicolor Statistics")
summary(iris[iris$Species == "versicolor",])
print("Virginica Statistics")
summary(iris[iris$Species == "virginica",])
test_model$centers
set.seed(1)
k <- 3
test_model <- kmeans(flower_features, centers = k, nstart = 20)
dataset$cluster_id <- factor(test_model$cluster)
ggplot(dataset, aes(Sepal.Length, Sepal.Width, color = cluster_id)) +
geom_point(alpha = 0.5) +
xlab("Sepal Length") +
ylab("Sepal Width")
#install.packages('caret')
library(caret)
confusionMatrix(factor(dataset$Species, levels = 1:3), factor(test_model$cluster, levels = 1:3))
print("Setosa Statistics")
summary(iris[iris$Species == "setosa",])
print("Versicolor Statistics")
summary(iris[iris$Species == "versicolor",])
print("Virginica Statistics")
summary(iris[iris$Species == "virginica",])
test_model$centers
print("Setosa Statistics")
summary(iris[iris$Species == "setosa",])
print("Versicolor Statistics")
summary(iris[iris$Species == "versicolor",])
print("Virginica Statistics")
summary(iris[iris$Species == "virginica",])
summary(dataset[dataset$cluster_id == 3,])
print("Setosa Statistics")
summary(iris[iris$Species == "setosa",])
print("Versicolor Statistics")
summary(iris[iris$Species == "versicolor",])
print("Virginica Statistics")
summary(iris[iris$Species == "virginica",])
summary(dataset[dataset$cluster_id == 3,1:4])
print("Setosa Statistics")
summary(iris[iris$Species == "setosa",])
print("Versicolor Statistics")
summary(iris[iris$Species == "versicolor",])
print("Virginica Statistics")
summary(iris[iris$Species == "virginica",])
summary(dataset[dataset$cluster_id == 3,])
print("Setosa Statistics and Cluster 3 Paired")
summary(iris[iris$Species == "setosa",])
summary(dataset[dataset$cluster_id == 3,])
print("Versicolor Statistics")
summary(iris[iris$Species == "versicolor",])
print("Virginica Statistics")
summary(iris[iris$Species == "virginica",])
print("Setosa Statistics and Cluster 3 Paired")
summary(iris[iris$Species == "setosa",])
summary(dataset[dataset$cluster_id == 3,])
print("Versicolor Statistics")
summary(iris[iris$Species == "versicolor",])
summary(dataset[dataset$cluster_id == 2,])
print("Virginica Statistics")
summary(iris[iris$Species == "virginica",])
summary(dataset[dataset$cluster_id == 1,])
print("Setosa Statistics and Cluster 3 Paired")
summary(iris[iris$Species == "setosa",])
summary(dataset[dataset$cluster_id == 3,])
print("Versicolor Statistics")
summary(iris[iris$Species == "versicolor",])
summary(dataset[dataset$cluster_id == 1,])
print("Virginica Statistics")
summary(iris[iris$Species == "virginica",])
summary(dataset[dataset$cluster_id == 2,])
set.seed(5)
k <- 3
test_model <- kmeans(flower_features, centers = k, nstart = 20)
dataset$cluster_id <- factor(test_model$cluster)
ggplot(dataset, aes(Sepal.Length, Sepal.Width, color = cluster_id)) +
geom_point(alpha = 0.5) +
xlab("Sepal Length") +
ylab("Sepal Width")
#install.packages('caret')
library(caret)
confusionMatrix(factor(dataset$Species, levels = 1:3), factor(test_model$cluster, levels = 1:3))
print("Setosa Statistics and Cluster 3 Paired")
summary(iris[iris$Species == "setosa",])
summary(dataset[dataset$cluster_id == 3,])
print("Versicolor Statistics")
summary(iris[iris$Species == "versicolor",])
summary(dataset[dataset$cluster_id == 1,])
print("Virginica Statistics")
summary(iris[iris$Species == "virginica",])
summary(dataset[dataset$cluster_id == 2,])
print("Setosa Statistics and Cluster 3 Paired")
summary(iris[iris$Species == "setosa",])
summary(dataset[dataset$cluster_id == 2,])
print("Versicolor Statistics")
summary(iris[iris$Species == "versicolor",])
summary(dataset[dataset$cluster_id == 1,])
print("Virginica Statistics")
summary(iris[iris$Species == "virginica",])
summary(dataset[dataset$cluster_id == 3,])
print("Setosa Statistics and Cluster 3 Paired")
summary(iris[iris$Species == "setosa",])
summary(dataset[dataset$cluster_id == 1,])
print("Versicolor Statistics")
summary(iris[iris$Species == "versicolor",])
summary(dataset[dataset$cluster_id == 2,])
print("Virginica Statistics")
summary(iris[iris$Species == "virginica",])
summary(dataset[dataset$cluster_id == 3,])
if (!require(httr)) install.packages("httr")
library(httr)
if (!require(jsonlite)) install.packages("jsonlite")
library(jsonlite)
capitolTradesurl <- "https://bff.capitoltrades.com/trades"
# Make a GET request to the API
response <- GET(capitolTradesurl)
# Check if the request was successful (status code 200)
if (http_status(response)$category == "Success") {
# Parse the JSON response
data <- fromJSON(content(response, "text"))
# Now you can work with the 'data' object
# For example, print the first few rows
print(head(data))
} else {
# If the request was not successful, print the error message
print(paste("Error:", http_status(response)$reason))
}
tail(data)
print(tail(data))
setwd("C:/Users/Chris/Downloads/ISYE6740_Project")
file = read.csv("merged_df.csv
")
file = read.csv("merged_df.csv")
head(file)
model = lm(California~CPI, data = file)
summary(model)
model = lm(CPI~California, data = file)
summary(model)
model = lm(California~CPI, data = file)
summary(model)
-159793 / 2508.7
model = lm(Texas~CPI, data = file)
summary(model)
